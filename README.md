# ğŸ™ï¸ Aivy: Your Voice Assistant Brought to Life! ğŸ¤–ğŸš€
Welcome to Aivy, an open-source voice assistant that channels the charm and intelligence of Tony Starkâ€™s iconic AI companion! Built with Python, Aivy combines real-time speech-to-text (STT), text-to-speech (TTS), and a local large language model (LLM) to deliver seamless, witty conversations. Whether youâ€™re asking about the weather or craving a clever quip, Aivy is here to assist with style! ğŸ˜
 
âœ¨ Features

ğŸ™ï¸ Real-Time Speech Recognition: Transcribes speech after 2 seconds of talking followed by 1.5 seconds of silence using faster_whisper.
ğŸ—£ï¸ Human-Like Speech Synthesis: Generates natural-sounding audio with the mimi TTS model (CSM-1B).
ğŸ§  Witty Conversations: Powered by LLaMA-3.2-1B via LM Studio, delivering Iron Man-style responses.
ğŸ”„ Overlap Prevention: Blocks new speech during responses for clear, uninterrupted interactions.
ğŸŒ Cross-Platform: Runs on Linux, macOS, and Windows with GPU or CPU support.

ğŸ› ï¸ Getting Started
Follow these steps to bring Aivy to life on your system. Whether youâ€™re on Linux, macOS, or Windows, Aivy is designed to be easy to set up and fun to use!
Prerequisites
Before you begin, ensure you have the following:

Python 3.8+: Check with python --version.
CUDA (Optional): For GPU acceleration (requires an NVIDIA GPU and CUDA toolkit).
FFmpeg: For audio processing.
Ubuntu: sudo apt install ffmpeg
macOS: brew install ffmpeg
Windows: Download from FFmpeg and add to your PATH.


LM Studio: To serve LLaMA-3.2-1B locally (download).
Models:
LLaMA-3.2-1B: Available via Hugging Face or LM Studio (requires a Hugging Face account).
CSM-1B: mimi weights from Hugging Face.


Microphone and Speakers: Any standard USB, Bluetooth, or built-in mic/speaker.
Git: To clone the repository (git --version).

Installation

Clone the Repository:
git clone https://github.com/yourusername/aivy.git
cd aivy


Set Up a Virtual Environment:
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate


Install Dependencies:
pip install -r requirements.txt

requirements.txt (included in the repo):
faster-whisper
openai
sounddevice
numpy
torch
torchaudio
transformers
huggingface_hub
safetensors
psutil


Download Models:

LLaMA-3.2-1B:
Download via LM Studio or Hugging Face (e.g., meta-llama/Llama-3.2-1B).
Set the model path environment variable:export LLAMA_MODEL_PATH=/path/to/Llama-3.2-1B




CSM-1B:
Download mimi weights from Hugging Face (e.g., moshi/mimi).
Set the model path:export CSM_MODEL_PATH=/path/to/csm-1b






Configure Audio Devices (Optional):

Set environment variables for your microphone and speaker:export INPUT_DEVICE=0  # Microphone index
export OUTPUT_DEVICE=0  # Speaker index


Run python -c "import sounddevice as sd; print(sd.query_devices())" to list available devices.
If not set, Aivy will prompt you to select devices interactively.


Set Environment (Linux Only):To ensure compatibility with audio libraries:
export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH



Running Aivy

Start LM Studio:

Open LM Studio and load the LLaMA-3.2-1B model.
Start the server at http://localhost:1234/v1 (default settings).


Launch Aivy:
python aivy.py


Select your microphone and speaker when prompted (or use INPUT_DEVICE/OUTPUT_DEVICE).
Speak: "Hey Aivy, whatâ€™s the weather?" and enjoy a witty response!


Test the Interaction:

Say: "Hey Aivy, tell me a joke."
Expected response: "Why did the AI go to therapy? It had an identity crisis, sir!"
Aivy will wait for 2 seconds of speech followed by 1.5 seconds of silence before transcribing.



Example Interaction
Aivy initializing...
Initialization complete in 2.35 seconds
Entering main loop...
Audio stream started...
Buffer size: 32000 samples, amplitude: 0.0450
Detected speech, updating last_speech_time
Waiting for silence (1500/1500ms)
Transcribing audio buffer...
User: Hey Aivy, what's the weather like today?
Sending to LLM...
Aivy: Sunny with a chance of brilliance, sir! (LLM took 0.20 seconds)
Generating speech for: Sunny with a chance...
Audio generation completed in 8.00 seconds

ğŸ› Known Issues

Audio Device Detection: If your mic or speaker isnâ€™t detected, check devices with python -c "import sounddevice as sd; print(sd.query_devices())" and set INPUT_DEVICE/OUTPUT_DEVICE accordingly.
Model Paths: Ensure LLAMA_MODEL_PATH and CSM_MODEL_PATH are set correctly to avoid loading errors.
Latency on CPU: GPU is recommended for real-time performance; CPU may introduce delays.
LM Studio Connection: If Aivy fails to connect to http://localhost:1234/v1, ensure LM Studio is running and the server is active.

File an issue on GitHub if you encounter problems, and include logs for faster resolution!
ğŸš€ Future Work & Improvements
Aivy is just the beginning! Hereâ€™s a roadmap to make it even more powerful:
Short-Term (1-3 Months)

ğŸ—£ï¸ Interruption Handling: Reintroduce real-time interruption, allowing users to stop playback by speaking (currently disabled for stability).
ğŸ›ï¸ GUI for Device Selection: Add a simple interface using tkinter or PyQt to choose audio devices.
âš¡ STT Optimization: Use smaller faster_whisper models (e.g., base) for faster transcription.

Medium-Term (3-6 Months)

ğŸ¤ Wake-Word Detection: Implement "Hey Aivy" to activate listening using webrtcvad or snowboy.
ğŸŒ Multilingual Support: Extend STT and TTS to non-English languages with whisper-large and multilingual TTS models.
ğŸ”‡ Noise Cancellation: Integrate rnnoise for robust performance in noisy environments.

Long-Term (6-12 Months)

â˜ï¸ Cloud LLM Integration: Support cloud-based LLMs (e.g., via xAIâ€™s API) for users without powerful hardware.
ğŸ“± Mobile App: Develop a mobile interface for Aivy using Kivy or Flutter.
ğŸ”Œ Plugin System: Enable custom commands (e.g., control smart home devices) via a plugin architecture.

ğŸ¤ Contributing
Weâ€™re thrilled to welcome contributions to Aivy! ğŸ™Œ Whether youâ€™re fixing bugs, adding features, or improving docs, your help will make Aivy the ultimate voice assistant.
How to Contribute

Fork the Repo: Click the "Fork" button on GitHub.
Create a Branch: git checkout -b my-feature.
Make Changes: Implement your feature or bug fix.
Commit: git commit -m "Add cool feature".
Push: git push origin my-feature.
Open a Pull Request: Describe your changes and submit!

Contribution Ideas

Debug and stabilize interruption handling (see Issues).
Add support for alternative TTS models (e.g., VITS, Tacotron).
Improve documentation with video tutorials or setup guides.
Optimize GPU memory usage for larger models.

Check the Issues tab for open tasks, or share your ideas in Discussions.
ğŸ“œ License
Aivy is licensed under the MIT License. See LICENSE for details.
ğŸ™ Acknowledgments
Aivy wouldnâ€™t be possible without these amazing projects:

faster_whisper: Lightning-fast speech recognition.
mimi (CSM-1B): High-quality text-to-speech synthesis.
LLaMA-3.2-1B: Powering Aivyâ€™s clever responses.
LM Studio: Seamless local LLM serving.
sounddevice: Reliable audio input and output.

Special thanks to the open-source community for inspiring and supporting Aivyâ€™s development! ğŸŒŸ
ğŸ“ Contact
Have questions or ideas? Reach out via GitHub Discussions or open an issue. Letâ€™s make Aivy legendary together!

â­ Star this repo if you love Aivy! Join us in building the ultimate voice assistant inspired by Iron Man! ğŸš€ğŸ¤–
